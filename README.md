# ğŸ‘‹ GestureDispatch

**Wave your hand at the webcam, get AI-powered code actions.** No typing, no buttons â€” just gestures.

Copy a code snippet, throw a hand sign, and an LLM streams back a fix, explanation, commit message, or test scaffold. Result auto-copies to your clipboard.

## How It Works

1. Copy a code snippet (or use the built-in demo)
2. Hold a hand gesture at your webcam for 0.8s
3. AI response streams in real-time
4. Result auto-copies to clipboard

## Gestures

| Key | Gesture | Action |
|-----|---------|--------|
| `1` | ğŸ‘ Thumbs down | Fix bugs |
| `2` | â˜ï¸ Point up | Explain code |
| `3` | âœŠ Fist | Write commit message |
| `4` | âœŒï¸ Peace | Scaffold tests |
| `0` | âœ‹ Open palm | Abort stream |

Keyboard shortcuts always work as fallback. Press `M` to mute sound cues.

## Tech Stack

Vanilla JS â€” no frameworks, no TypeScript. **Vite** bundles it, **MediaPipe Hands** tracks your fingers, **p5.js** draws a stylized hand skeleton with a confidence ring and particle bursts, and **Gemini 2.5 Flash** (via OpenRouter) handles the AI. One HTML page, zero npm dependencies beyond Vite.

## Setup

```bash
npm install
cp .env.example .env       # add your OPENROUTER_API_KEY
npm run dev
```

## Team

**Samuel Kahessay** â€” AI pipeline, streaming output, clipboard integration

**Bishesh Khanal** â€” Hand detection, gesture classification, p5.js visualization, sound design

---

*Built at Cursor Meetup Calgary, February 2026*
