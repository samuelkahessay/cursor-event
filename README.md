# GestureDispatch (SignalFire)

AI-powered code snippets triggered by hand gestures. Select code, make a hand sign at your webcam, and the gesture dispatches a Claude API call with that code as context. The result streams into an output panel and auto-copies to clipboard.

**Core loop:** Select code â†’ Gesture â†’ Claude API â†’ Stream result â†’ Clipboard

## Demo

1. Copy a code snippet (or use the built-in demo code)
2. Make a hand gesture at your webcam
3. Watch the AI response stream in real-time
4. Result auto-copies to your clipboard

## Gestures

| Key | Gesture | Action |
|-----|---------|--------|
| `1` | ğŸ‘ Thumbs down | Fix bugs |
| `2` | â˜ï¸ Point up | Explain code |
| `3` | âœŠ Fist | Write commit message |
| `4` | âœŒï¸ Peace | Scaffold tests |
| `0` | âœ‹ Open palm | Abort stream |

Gestures require a 0.8s hold before dispatch fires. Keyboard shortcuts are always available as a fallback.

## Tech Stack

| Layer | Choice |
|-------|--------|
| Bundler | Vite |
| Language | Vanilla JS |
| Hand Detection | MediaPipe Hands (CDN) |
| AI | Claude via OpenRouter (`anthropic/claude-sonnet-4`) |
| Visualization | p5.js (CDN) â€” stylized hand landmark rendering |
| UI | Single HTML page + CSS |

## Project Structure

```
â”œâ”€â”€ index.html              â† layout: camera left, output + activity log right
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.js             â† entry: camera init, MediaPipe, gesture loop, keyboard fallback
â”‚   â”œâ”€â”€ classifier.js       â† landmark â†’ gesture classifier
â”‚   â”œâ”€â”€ dispatcher.js       â† dispatch(gesture, code) â†’ Claude API â†’ stream â†’ clipboard
â”‚   â”œâ”€â”€ output-panel.js     â† streaming output DOM controller
â”‚   â”œâ”€â”€ activity-log.js     â† timestamped activity log (bottom-right panel)
â”‚   â”œâ”€â”€ api.js              â† OpenRouter SSE streaming
â”‚   â”œâ”€â”€ prompts.js          â† prompt templates, status messages, fallback data
â”‚   â”œâ”€â”€ handviz.js          â† p5.js hand skeleton, confidence ring, particles
â”‚   â”œâ”€â”€ sounds.js           â† Web Audio API tone generator
â”‚   â”œâ”€â”€ camera.js           â† webcam setup
â”‚   â”œâ”€â”€ mediapipe.js        â† MediaPipe Hands init
â”‚   â”œâ”€â”€ ui.js               â† gesture feedback UI
â”‚   â””â”€â”€ styles.css          â† all styling
â”œâ”€â”€ api/
â”‚   â””â”€â”€ chat.js             â† Vercel Edge Function (production API proxy)
â”œâ”€â”€ vite.config.js          â† dev server config + API proxy
â””â”€â”€ .env                    â† OPENROUTER_API_KEY (gitignored)
```

## Setup

```bash
npm install
cp .env.example .env       # add your OPENROUTER_API_KEY
npm run dev                 # starts Vite dev server
```

## Layout

- **Left:** Camera feed with p5.js hand visualization overlay (skeleton, confidence ring, particle bursts)
- **Top right:** Code output panel â€” streams AI responses with syntax highlighting
- **Bottom right:** Activity log â€” timestamped events for gestures, dispatches, clipboard copies, and errors

## Team

- **Bishesh** â€” Detection & UI (camera, MediaPipe, gesture classifier, p5.js visualization, sound)
- **Sam** â€” AI Pipeline & Output (dispatcher, Claude API streaming, output panel, clipboard)
